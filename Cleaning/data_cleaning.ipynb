{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files in the Details folder\n",
    "details_path = \"../Data/Input/Events/\"\n",
    "files = [f\"{details_path}{filename}\" for filename in os.listdir(details_path) if os.path.isfile((os.path.join(details_path, filename)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"../Data/Output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CZ_TIMEZONE</th>\n",
       "      <th>CZ_TIMEZONE_RECODE</th>\n",
       "      <th>UTC_OFFSET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CST</td>\n",
       "      <td>CST-6</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MST</td>\n",
       "      <td>MST-7</td>\n",
       "      <td>-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EST</td>\n",
       "      <td>EST-5</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PST</td>\n",
       "      <td>PST-8</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNK</td>\n",
       "      <td>UNK</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CDT</td>\n",
       "      <td>CDT-5</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EDT</td>\n",
       "      <td>EDT-4</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MDT</td>\n",
       "      <td>MDT-6</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GMT</td>\n",
       "      <td>GMT-0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HST</td>\n",
       "      <td>HST-10</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PDT</td>\n",
       "      <td>PDT-7</td>\n",
       "      <td>-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CSC</td>\n",
       "      <td>CST-6</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AST</td>\n",
       "      <td>AST-4</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>EST-5</td>\n",
       "      <td>EST-5</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MST-7</td>\n",
       "      <td>MST-7</td>\n",
       "      <td>-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CST-6</td>\n",
       "      <td>CST-6</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PST-8</td>\n",
       "      <td>PST-8</td>\n",
       "      <td>-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AST-4</td>\n",
       "      <td>AST-4</td>\n",
       "      <td>-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HST-10</td>\n",
       "      <td>HST-10</td>\n",
       "      <td>-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AKST-9</td>\n",
       "      <td>AKST-9</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CZ_TIMEZONE CZ_TIMEZONE_RECODE  UTC_OFFSET\n",
       "0          CST              CST-6          -6\n",
       "1          MST              MST-7          -7\n",
       "2          EST              EST-5          -5\n",
       "3          PST              PST-8          -8\n",
       "4          UNK                UNK           0\n",
       "5          CDT              CDT-5          -5\n",
       "6          EDT              EDT-4          -4\n",
       "7          MDT              MDT-6          -6\n",
       "8          GMT              GMT-0           0\n",
       "9          HST             HST-10         -10\n",
       "10         PDT              PDT-7          -7\n",
       "11         CSC              CST-6          -6\n",
       "12         AST              AST-4          -4\n",
       "13       EST-5              EST-5          -5\n",
       "14       MST-7              MST-7          -7\n",
       "15       CST-6              CST-6          -6\n",
       "16       PST-8              PST-8          -8\n",
       "17       AST-4              AST-4          -4\n",
       "18      HST-10             HST-10         -10\n",
       "19      AKST-9             AKST-9          -9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Timezone data\n",
    "tz_codes = ['CST', 'MST', 'EST', 'PST', 'UNK', 'CDT', 'EDT', 'MDT', 'GMT', 'HST', 'PDT', 'CSC',\n",
    " 'AST', 'EST-5', 'MST-7', 'CST-6', 'PST-8', 'AST-4', 'HST-10', 'AKST-9']\n",
    "tz_offsets = [-6, -7, -5, -8, 0, -5, -4, -6, 0, -10, -7, -6, -4, -5, -7, -6, -8, -4, -10, -9]\n",
    "tz_recode = ['CST-6', 'MST-7', 'EST-5', 'PST-8', 'UNK', 'CDT-5', 'EDT-4', 'MDT-6', 'GMT-0',\n",
    "             'HST-10', 'PDT-7', 'CST-6', 'AST-4', 'EST-5', 'MST-7', 'CST-6', 'PST-8', 'AST-4',\n",
    "             'HST-10', 'AKST-9']\n",
    "timezones_df = pd.DataFrame({\n",
    "    \"CZ_TIMEZONE\": tz_codes,\n",
    "    \"CZ_TIMEZONE_RECODE\": tz_recode,\n",
    "    \"UTC_OFFSET\": tz_offsets\n",
    "})\n",
    "\n",
    "timezones_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file):\n",
    "    print(f\"Processing file {file}\")\n",
    "    # Read file. Set FIPS columns to string\n",
    "    detail_data_raw_df = pd.read_csv(file, dtype={\"STATE_FIPS\": str,\n",
    "                                                  \"CZ_FIPS\": str,\n",
    "                                                  \"TOR_OTHER_CZ_FIPS\": str,\n",
    "                                                  \"DAMAGE_PROPERTY\": str,\n",
    "                                                  \"DAMAGE_CROPS\": str,\n",
    "                                                  \"BEGIN_AZIMUTH\": str,\n",
    "                                                  \"BEGIN_LOCATION\": str,\n",
    "                                                  \"END_AZIMUTH\": str,\n",
    "                                                  \"END_LOCATION\": str})\n",
    "    \n",
    "    # Filter tornadoes\n",
    "    detail_data_clean_df = detail_data_raw_df[detail_data_raw_df[\"EVENT_TYPE\"] == \"Tornado\"]\n",
    "    \n",
    "    # Renumber Puerto Rico & Virgin Islands state FIPS\n",
    "    detail_data_clean_df.loc[detail_data_clean_df[\"STATE\"] == \"PUERTO RICO\", \"STATE_FIPS\"] = \"72\"\n",
    "    detail_data_clean_df.loc[detail_data_clean_df[\"STATE\"] == \"VIRGIN ISLANDS\", \"STATE_FIPS\"] = \"78\"\n",
    "\n",
    "    # Combine State and County FIPS\n",
    "    detail_data_clean_df[\"FIPS\"] = (detail_data_clean_df[\"STATE_FIPS\"].str.zfill(2)) + (detail_data_clean_df[\"CZ_FIPS\"].str.zfill(3))\n",
    "    \n",
    "    # Process timestamps\n",
    "    pattern = r'-\\d{2}'\n",
    "    detail_data_clean_df[\"YEAR\"] = detail_data_clean_df[\"YEAR\"].astype(str)\n",
    " \n",
    "    # FROM MICROSOFT COPILOT\n",
    "    # Replace 2-digit year with 4-digit year for timestamp creation\n",
    "    detail_data_clean_df[\"BEGIN_DATE_TIME\"] = detail_data_clean_df.apply(\n",
    "        lambda row: pd.Series(row[\"BEGIN_DATE_TIME\"]).str.replace(pattern, f\"-{row[\"YEAR\"]}\", regex=True)[0],axis=1)\n",
    "    detail_data_clean_df[\"END_DATE_TIME\"] = detail_data_clean_df.apply(\n",
    "        lambda row: pd.Series(row[\"END_DATE_TIME\"]).str.replace(pattern, f\"-{row[\"YEAR\"]}\", regex=True)[0],axis=1)\n",
    "    \n",
    "    # Convert BEGIN_DATE_TIME and END_DATE_TIME to UTC UNIX (POSIX) timestamps\n",
    "    detail_data_clean_df.loc[:,\"B_DATE_TIME\"] = pd.to_datetime(detail_data_clean_df.loc[:,\"BEGIN_DATE_TIME\"],\n",
    "                                                               format=\"%d-%b-%Y %H:%M:%S\")\n",
    "    detail_data_clean_df.loc[:,\"E_DATE_TIME\"] = pd.to_datetime(detail_data_clean_df.loc[:,\"END_DATE_TIME\"],\n",
    "                                                               format=\"%d-%b-%Y %H:%M:%S\")\n",
    "    \n",
    "    detail_data_clean_df = detail_data_clean_df.merge(timezones_df, on=\"CZ_TIMEZONE\", how=\"left\")\n",
    "\n",
    "    detail_data_clean_df[\"B_DATE_TIME\"] = detail_data_clean_df[\"B_DATE_TIME\"]\\\n",
    "                                        - pd.TimedeltaIndex(detail_data_clean_df[\"UTC_OFFSET\"], unit=\"H\")\n",
    "    detail_data_clean_df[\"E_DATE_TIME\"] = detail_data_clean_df[\"E_DATE_TIME\"]\\\n",
    "                                        - pd.TimedeltaIndex(detail_data_clean_df[\"UTC_OFFSET\"], unit=\"H\")\n",
    "    \n",
    "    detail_data_clean_df.loc[:,\"BEGIN_TIMESTAMP\"] = detail_data_clean_df.loc[:,\"B_DATE_TIME\"].astype(\"int64\") // 10**9\n",
    "    detail_data_clean_df.loc[:,\"END_TIMESTAMP\"] = detail_data_clean_df.loc[:,\"E_DATE_TIME\"].astype(\"int64\") // 10**9\n",
    "\n",
    "    # Get F/EF scale codes\n",
    "    detail_data_clean_df[\"TOR_F_SCALE\"] = detail_data_clean_df[\"TOR_F_SCALE\"].fillna(\"EFU\")\n",
    "    pattern = r'F(\\w)'\n",
    "    detail_data_clean_df[\"TOR_F_LEVEL\"] = detail_data_clean_df[\"TOR_F_SCALE\"].str.extract(pattern)\n",
    "\n",
    "    # Accumulate Deaths & Injuries\n",
    "    detail_data_clean_df[\"DEATHS\"] = detail_data_clean_df[\"DEATHS_DIRECT\"] + detail_data_clean_df[\"DEATHS_INDIRECT\"]\n",
    "    detail_data_clean_df[\"INJURIES\"] = detail_data_clean_df[\"INJURIES_DIRECT\"] + detail_data_clean_df[\"INJURIES_INDIRECT\"]\n",
    "\n",
    "    # Convert DAMAGE_PROPERTY to numeric\n",
    "    detail_data_clean_df.loc[:,\"DAMAGE_PROPERTY\"] = detail_data_clean_df.loc[:,\"DAMAGE_PROPERTY\"].fillna(\"0.00K\")\n",
    "    detail_data_clean_df.loc[detail_data_clean_df[\"DAMAGE_PROPERTY\"].str.match(r'\\d*[.]?\\d*[^KMB]\\Z'), \"DAMAGE_PROPERTY\"] += \"K\"\n",
    "\n",
    "    pattern = r'(\\d*[.]?\\d*)[KMB]'\n",
    "    detail_data_clean_df[\"DMG_PRP\"] = detail_data_clean_df[\"DAMAGE_PROPERTY\"].str.extract(pattern).astype(float)\n",
    "    pattern = r'\\d*[.]?\\d*([KMB])'\n",
    "    detail_data_clean_df[\"DMG_PRP_MULT_STR\"] = detail_data_clean_df[\"DAMAGE_PROPERTY\"].str.extract(pattern)\n",
    "    detail_data_clean_df[\"DMG_PRP\"] = (detail_data_clean_df[\"DMG_PRP\"] * \n",
    "                                        np.where(detail_data_clean_df[\"DMG_PRP_MULT_STR\"] == \"K\", 1000, 1))\n",
    "    detail_data_clean_df[\"DMG_PRP\"] = (detail_data_clean_df[\"DMG_PRP\"] * \n",
    "                                        np.where(detail_data_clean_df[\"DMG_PRP_MULT_STR\"] == \"M\", 1000000, 1))\n",
    "    detail_data_clean_df[\"DMG_PRP\"] = (detail_data_clean_df[\"DMG_PRP\"] * \n",
    "                                        np.where(detail_data_clean_df[\"DMG_PRP_MULT_STR\"] == \"B\", 1000000000, 1))\n",
    "\n",
    "    # Convert DAMAGE_CROPS to numeric\n",
    "    detail_data_clean_df.loc[:,\"DAMAGE_CROPS\"] = detail_data_clean_df.loc[:,\"DAMAGE_CROPS\"].fillna(\"0.00K\")\n",
    "    detail_data_clean_df.loc[detail_data_clean_df[\"DAMAGE_CROPS\"].str.match(r'\\d*[.]?\\d*[^KMB]\\Z'), \"DAMAGE_CROPS\"] += \"K\"\n",
    "\n",
    "    pattern = r'(\\d*[.]?\\d*)[KMB]'\n",
    "    detail_data_clean_df[\"DMG_CRP\"] = detail_data_clean_df[\"DAMAGE_CROPS\"].str.extract(pattern).astype(float)\n",
    "    pattern = r'\\d*[.]?\\d*([KMB])'\n",
    "    detail_data_clean_df[\"DMG_CRP_MULT_STR\"] = detail_data_clean_df[\"DAMAGE_CROPS\"].str.extract(pattern)\n",
    "    detail_data_clean_df[\"DMG_CRP\"] = (detail_data_clean_df[\"DMG_CRP\"] * \n",
    "                                        np.where(detail_data_clean_df[\"DMG_CRP_MULT_STR\"] == \"K\", 1000, 1))\n",
    "    detail_data_clean_df[\"DMG_CRP\"] = (detail_data_clean_df[\"DMG_CRP\"] * \n",
    "                                        np.where(detail_data_clean_df[\"DMG_CRP_MULT_STR\"] == \"M\", 1000000, 1))\n",
    "    detail_data_clean_df[\"DMG_CRP\"] = (detail_data_clean_df[\"DMG_CRP\"] * \n",
    "                                        np.where(detail_data_clean_df[\"DMG_CRP_MULT_STR\"] == \"B\", 1000000000, 1))\n",
    "    \n",
    "    # Trim columns\n",
    "    detail_data_clean_df = detail_data_clean_df[[\n",
    "       'EVENT_ID', 'FIPS',\n",
    "       'BEGIN_TIMESTAMP', 'END_TIMESTAMP',\n",
    "       'DEATHS', 'INJURIES', 'DMG_PRP', 'DMG_CRP',\n",
    "       'TOR_F_SCALE', 'TOR_F_LEVEL', 'TOR_LENGTH', 'TOR_WIDTH',        \n",
    "       'BEGIN_RANGE', 'BEGIN_AZIMUTH', 'BEGIN_LOCATION',\n",
    "       'END_RANGE', 'END_AZIMUTH', 'END_LOCATION',\n",
    "       'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON',\n",
    "       'EVENT_NARRATIVE']]\n",
    "    \n",
    "    # Rename temporary columns back to their original names\n",
    "    detail_data_clean_df = detail_data_clean_df.rename({\"DMG_PRP\": \"DAMAGE_PROPERTY\",\n",
    "                                                        \"DMG_CRP\": \"DAMAGE_CROPS\" }, axis=1)\n",
    "\n",
    "    # Fix numeric dtypes\n",
    "    detail_data_clean_df[\"DAMAGE_PROPERTY\"] = detail_data_clean_df[\"DAMAGE_PROPERTY\"].astype(\"int64\")\n",
    "    detail_data_clean_df[\"DAMAGE_CROPS\"] = detail_data_clean_df[\"DAMAGE_CROPS\"].astype(\"int64\")\n",
    "\n",
    "    return detail_data_clean_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1950_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1951_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1952_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1953_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1954_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1955_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1956_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1957_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1958_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1959_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1960_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1961_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1962_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1963_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1964_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1965_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1966_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1967_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1968_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1969_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1970_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1971_c20210803.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1972_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1973_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1974_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1975_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1976_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1977_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1978_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1979_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1980_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1981_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1982_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1983_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1984_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1985_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1986_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1987_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1988_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1989_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1990_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1991_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1992_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1993_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1994_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1995_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1996_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1997_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1998_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d1999_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2000_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2001_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2002_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2003_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2004_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2005_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2006_c20250122.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2007_c20240216.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2008_c20240620.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2009_c20231116.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2010_c20220425.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2011_c20230417.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2012_c20221216.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2013_c20230118.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2014_c20231116.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2015_c20240716.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2016_c20220719.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2017_c20250122.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2018_c20240716.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2019_c20240117.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2020_c20240620.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2021_c20240716.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2022_c20241121.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2023_c20241216.csv\n",
      "Processing file ../Data/Details/StormEvents_details-ftp_v1.0_d2024_c20250122.csv\n",
      "File processing complete\n",
      "File written to ../Data/Cleaned//Tornadoes_1950_2024.csv\n",
      "File written to ../Data/Cleaned//Tornadoes_1950_2024.json\n"
     ]
    }
   ],
   "source": [
    "enabled = True\n",
    "write = True\n",
    "\n",
    "if enabled:\n",
    "    df_list = [process_file(file) for file in files]\n",
    "    details_full_df = pd.concat(df_list)\n",
    "    details_full_df = details_full_df.sort_values(\"BEGIN_TIMESTAMP\")\n",
    "    details_full_df = details_full_df.reset_index()\n",
    "    details_full_df = details_full_df.drop(columns=[\"index\"])\n",
    "    print(\"File processing complete\")\n",
    "\n",
    "    if write:\n",
    "        details_full_df.to_csv(f\"{output_path}/Tornadoes_1950_2024.csv\", index=False)        \n",
    "        print(f\"File written to {output_path}/Tornadoes_1950_2024.csv\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>BEGIN_TIMESTAMP</th>\n",
       "      <th>END_TIMESTAMP</th>\n",
       "      <th>DEATHS</th>\n",
       "      <th>INJURIES</th>\n",
       "      <th>DAMAGE_PROPERTY</th>\n",
       "      <th>DAMAGE_CROPS</th>\n",
       "      <th>TOR_F_SCALE</th>\n",
       "      <th>TOR_F_LEVEL</th>\n",
       "      <th>...</th>\n",
       "      <th>BEGIN_AZIMUTH</th>\n",
       "      <th>BEGIN_LOCATION</th>\n",
       "      <th>END_RANGE</th>\n",
       "      <th>END_AZIMUTH</th>\n",
       "      <th>END_LOCATION</th>\n",
       "      <th>BEGIN_LAT</th>\n",
       "      <th>BEGIN_LON</th>\n",
       "      <th>END_LAT</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [EVENT_ID, FIPS, BEGIN_TIMESTAMP, END_TIMESTAMP, DEATHS, INJURIES, DAMAGE_PROPERTY, DAMAGE_CROPS, TOR_F_SCALE, TOR_F_LEVEL, TOR_LENGTH, TOR_WIDTH, BEGIN_RANGE, BEGIN_AZIMUTH, BEGIN_LOCATION, END_RANGE, END_AZIMUTH, END_LOCATION, BEGIN_LAT, BEGIN_LON, END_LAT, END_LON, EVENT_NARRATIVE]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 23 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the FIPS columns matches the current FIPS data\n",
    "fips_path = \"../Data/Output/fips_data.csv\"\n",
    "fips_df = pd.read_csv(fips_path, dtype=str)\n",
    "fips_list = fips_df[\"FIPS\"].tolist()\n",
    "\n",
    "details_full_df[~details_full_df[\"FIPS\"].isin(fips_list)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
